{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb63c9ad-db56-40f6-9e90-38ccdc272b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc89f9-9d6d-4647-bc80-e9499aa297a2",
   "metadata": {},
   "source": [
    "# Load MNSIT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab736f96-95ca-4fb0-9e6b-ee87ca717e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torchvision.datasets.MNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# Implementing DataLoaders\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = 64, shuffle = True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "733ae555-cf65-439b-87d5-f7874e4b321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape:  torch.Size([64, 1, 28, 28])\n",
      "train_label shape:  torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_label = next(iter(train_dataloader))\n",
    "\n",
    "print(\"train_features shape: \", train_features.size())\n",
    "print(\"train_label shape: \", train_label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a27750d0-0924-478c-bb01-8444866835f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = train_features[0].squeeze()\n",
    "label = train_label[0]\n",
    "\n",
    "plt.title(f\"{label}\")\n",
    "plt.imshow(img, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213711c6-826e-4a21-835b-a0328f164c3b",
   "metadata": {},
   "source": [
    "# Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8a5f6ca-c675-4423-9a46-8567e89b74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training = self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b92a364-c5d3-40b2-a84a-03169ea823a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.01,\n",
    "                      momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "173e6d8e-26f6-4f0a-bcc6-4d6a21f61597",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_dataloader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f52c941-d915-4fab-a632-a1f05d5223bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % 10 == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "        100. * batch_idx / len(train_dataloader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_dataloader.dataset)))\n",
    "      torch.save(network.state_dict(), 'model.pth')\n",
    "      torch.save(optimizer.state_dict(), 'optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f829c03d-f56a-4cf2-8409-fd5883eb5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_dataloader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_dataloader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_dataloader.dataset),\n",
    "    100. * correct / len(test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8cec09b-5b26-441d-89b5-ccca3957eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_f/6j9gn6dj19xftmqy10_2zr7m0000gn/T/ipykernel_89082/420148554.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3072, Accuracy: 911/10000 (9%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313809\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.323344\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.297519\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.286045\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.274732\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.294171\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.299711\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.329440\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.289531\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.287443\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.302925\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.297152\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.288104\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.291480\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.272311\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.263172\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.274955\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.268209\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.266747\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.251733\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.244319\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.222179\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.220809\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.275359\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.241120\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.203995\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.160293\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.152875\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.109618\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.088911\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.131999\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.023170\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.971644\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.971040\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.742333\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.855314\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.812499\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 1.541960\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.428062\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 1.496772\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.528183\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 1.482641\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.180516\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 1.255800\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.244446\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.123535\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.963686\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 1.117759\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.033156\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.945644\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.066201\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 1.029733\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.829829\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 1.110677\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.861869\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.067262\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.894513\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.998195\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.769242\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.746990\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.750349\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.881713\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.108002\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.859134\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.786547\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.821354\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.953173\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.704713\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.703659\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.623515\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.798703\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.835670\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.692421\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.897912\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.627843\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.731227\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.693598\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.727859\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.636110\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.659387\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.656491\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.491515\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.667969\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.605034\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.651379\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.850292\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.879427\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.637317\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.612190\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.548568\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.711851\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.879431\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.597165\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.644777\n",
      "\n",
      "Test set: Avg. loss: 0.3298, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.586580\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.616445\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.642493\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.664102\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.379088\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.628287\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.810579\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.597080\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.567842\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.599746\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.364925\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.445426\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.556854\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.440016\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.616734\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.765254\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.529163\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.607977\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.474648\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.654241\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.455361\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.556690\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.664093\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.649681\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.681990\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.516344\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.567936\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.600361\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.811376\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.370320\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.630765\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.406367\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.589811\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.425172\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.636906\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.732213\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.887082\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.557893\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.398364\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.449229\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.510419\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.316796\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.514003\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.296919\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.529461\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.490608\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.552596\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.567952\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.607671\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.388945\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.622269\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.301738\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.404813\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.512022\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.399115\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.512891\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.522024\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.369902\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.421828\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.457426\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.510793\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.477759\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.443698\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.530693\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.634671\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.436818\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.366876\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.743674\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.581123\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.382503\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.259321\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.389720\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.432381\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.414676\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.490400\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.522481\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.310912\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.502895\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.459490\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.546042\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.461725\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.477954\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.369511\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.680847\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.650963\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.566343\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.487273\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.600126\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.282254\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.450359\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.453523\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.440567\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.379638\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.321392\n",
      "\n",
      "Test set: Avg. loss: 0.1829, Accuracy: 9458/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.223703\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.277945\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.419239\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.362245\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.391021\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.440064\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.303146\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.523281\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.308306\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.309273\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.366759\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.515299\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.577888\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.291095\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.221399\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.672993\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.295871\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.519922\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.272491\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.395403\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.482540\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.355778\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.403373\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.440176\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.451386\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.403489\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.486227\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.461527\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.444512\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.483370\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.536873\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.447649\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.442386\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.735246\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.308524\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.291722\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.216897\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.480563\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.447708\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.413496\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.305436\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.610072\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.331839\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.469233\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.554140\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.351002\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.325031\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.414932\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.269670\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.455475\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.605340\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.373470\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.254952\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.440081\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.391285\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.326879\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.204248\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.275116\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.560258\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.150281\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.340072\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.397890\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.393840\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.386258\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.279340\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.399840\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.351775\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.695120\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.222513\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.407190\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.367532\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.483838\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.269426\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.420817\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.452664\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.273241\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.231500\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.298358\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.397504\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.431764\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.267094\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.315619\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.346719\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.507415\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.497879\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.275352\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.429131\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.507902\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.337335\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.613927\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.200001\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.425523\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.223196\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.296765\n",
      "\n",
      "Test set: Avg. loss: 0.1439, Accuracy: 9553/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999962b5-4ada-4158-bd1e-b9536b68f7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
